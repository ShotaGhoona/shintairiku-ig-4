#!/usr/bin/env python3
"""
Historical Data Collection Script
éå»ã®æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¸€æ‹¬åé›†ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Usage:
    # éå»30æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿åé›†
    python scripts/collect_historical_data.py --account 17841402015304577 --days 30

    # æŒ‡å®šæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿åé›†
    python scripts/collect_historical_data.py --account 17841402015304577 --from 2024-01-01 --to 2024-12-31

    # å…¨æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿åé›†
    python scripts/collect_historical_data.py --account 17841402015304577 --all-posts

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã¿åé›†
    python scripts/collect_historical_data.py --account 17841402015304577 --missing-metrics

    # æŠ•ç¨¿ã®ã¿ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ãªã—ï¼‰
    python scripts/collect_historical_data.py --account 17841402015304577 --days 30 --no-metrics
"""

import asyncio
import sys
import argparse
import logging
import os
from datetime import datetime, date, timedelta
from typing import Optional
import json

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from app.services.data_collection.historical_collector_service import create_historical_collector
from app.core.database import test_connection

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('historical_collection.log', mode='a')
    ]
)
logger = logging.getLogger(__name__)

def parse_arguments():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ"""
    parser = argparse.ArgumentParser(
        description='Instagram Historical Data Collection Script',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # éå»30æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿åé›†
  python scripts/collect_historical_data.py --account 17841402015304577 --days 30

  # æŒ‡å®šæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿åé›†
  python scripts/collect_historical_data.py --account 17841402015304577 --from 2024-01-01 --to 2024-12-31

  # å…¨æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹å«ã‚€ï¼‰
  python scripts/collect_historical_data.py --account 17841402015304577 --all-posts

  # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒæ¬ æã—ã¦ã„ã‚‹æŠ•ç¨¿ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã¿åé›†
  python scripts/collect_historical_data.py --account 17841402015304577 --missing-metrics

  # æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã®ã¿åé›†ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ãªã—ï¼‰
  python scripts/collect_historical_data.py --account 17841402015304577 --days 30 --no-metrics

  # æœ€å¤§100æŠ•ç¨¿ã¾ã§
  python scripts/collect_historical_data.py --account 17841402015304577 --all-posts --max-posts 100

  # è©³ç´°ãƒ­ã‚°å‡ºåŠ›
  python scripts/collect_historical_data.py --account 17841402015304577 --days 7 --verbose
        """
    )
    
    parser.add_argument(
        '--account',
        type=str,
        required=True,
        help='Instagram User ID (å¿…é ˆ)',
        metavar='USER_ID'
    )
    
    # æœŸé–“æŒ‡å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆæ’ä»–çš„ï¼‰
    date_group = parser.add_mutually_exclusive_group()
    date_group.add_argument(
        '--days',
        type=int,
        help='éå»Næ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿åé›†',
        metavar='N'
    )
    
    date_group.add_argument(
        '--all-posts',
        action='store_true',
        help='å…¨æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿åé›†'
    )
    
    date_group.add_argument(
        '--missing-metrics',
        action='store_true',
        help='ãƒ¡ãƒˆãƒªã‚¯ã‚¹æœªå–å¾—æŠ•ç¨¿ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã¿åé›†'
    )
    
    parser.add_argument(
        '--from',
        dest='from_date',
        type=str,
        help='é–‹å§‹æ—¥ä»˜ (YYYY-MM-DDå½¢å¼)',
        metavar='YYYY-MM-DD'
    )
    
    parser.add_argument(
        '--to',
        dest='to_date',
        type=str,
        help='çµ‚äº†æ—¥ä»˜ (YYYY-MM-DDå½¢å¼)',
        metavar='YYYY-MM-DD'
    )
    
    parser.add_argument(
        '--max-posts',
        type=int,
        help='æœ€å¤§æŠ•ç¨¿æ•°åˆ¶é™',
        metavar='N'
    )
    
    parser.add_argument(
        '--no-metrics',
        action='store_true',
        help='ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—'
    )
    
    parser.add_argument(
        '--chunk-size',
        type=int,
        default=50,
        help='ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 50ï¼‰',
        metavar='N'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='è©³ç´°ãƒ­ã‚°å‡ºåŠ›'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        help='çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›',
        metavar='output.json'
    )
    
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Ÿè¡Œï¼ˆAPIå‘¼ã³å‡ºã—ã®ã¿ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ãªã—ï¼‰'
    )
    
    parser.add_argument(
        '-y', '--yes',
        action='store_true',
        help='ç¢ºèªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—'
    )
    
    return parser.parse_args()

def validate_date(date_string: str) -> date:
    """æ—¥ä»˜æ–‡å­—åˆ—ã®æ¤œè¨¼"""
    try:
        return datetime.strptime(date_string, '%Y-%m-%d').date()
    except ValueError:
        raise argparse.ArgumentTypeError(f"Invalid date format: {date_string}. Use YYYY-MM-DD format.")

def setup_logging(verbose: bool):
    """ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®š"""
    if verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        logging.getLogger('app').setLevel(logging.DEBUG)
        logger.info("Verbose logging enabled")

def calculate_date_range(args) -> tuple[Optional[date], Optional[date]]:
    """å¼•æ•°ã‹ã‚‰æ—¥ä»˜ç¯„å›²ã‚’è¨ˆç®—"""
    start_date = None
    end_date = None
    
    if args.days:
        end_date = date.today()
        start_date = end_date - timedelta(days=args.days)
    elif args.from_date or args.to_date:
        if args.from_date:
            start_date = validate_date(args.from_date)
        if args.to_date:
            end_date = validate_date(args.to_date)
    elif args.all_posts:
        # å…¨æŠ•ç¨¿ï¼ˆæ—¥ä»˜åˆ¶é™ãªã—ï¼‰
        pass
    elif args.missing_metrics:
        # éå»30æ—¥é–“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹æœªå–å¾—æŠ•ç¨¿
        end_date = date.today()
        start_date = end_date - timedelta(days=30)
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: éå»7æ—¥é–“
        end_date = date.today()
        start_date = end_date - timedelta(days=7)
        logger.info("No date range specified, defaulting to last 7 days")
    
    return start_date, end_date

def estimate_duration(
    total_posts: Optional[int],
    include_metrics: bool,
    chunk_size: int
) -> str:
    """å‡¦ç†æ™‚é–“ã®è¦‹ç©ã‚‚ã‚Š"""
    if not total_posts:
        return "Unknown"
    
    # åŸºæœ¬å‡¦ç†æ™‚é–“ï¼ˆæŠ•ç¨¿ã‚ãŸã‚Š0.5ç§’ï¼‰
    base_time = total_posts * 0.5
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—æ™‚é–“ï¼ˆæŠ•ç¨¿ã‚ãŸã‚Š1ç§’ï¼‰
    if include_metrics:
        base_time += total_posts * 1.0
    
    # ãƒãƒ£ãƒ³ã‚¯é–“å¾…æ©Ÿæ™‚é–“
    chunks = (total_posts + chunk_size - 1) // chunk_size
    chunk_delay = (chunks - 1) * 2  # ãƒãƒ£ãƒ³ã‚¯é–“2ç§’å¾…æ©Ÿ
    
    total_seconds = base_time + chunk_delay
    
    if total_seconds < 60:
        return f"{total_seconds:.0f} seconds"
    elif total_seconds < 3600:
        return f"{total_seconds/60:.1f} minutes"
    else:
        return f"{total_seconds/3600:.1f} hours"

def print_collection_plan(args, start_date: Optional[date], end_date: Optional[date]):
    """åé›†è¨ˆç”»ã®è¡¨ç¤º"""
    print("\n" + "="*60)
    print("ğŸ“Š HISTORICAL DATA COLLECTION PLAN")
    print("="*60)
    
    print(f"ğŸ¯ Account: {args.account}")
    
    if args.missing_metrics:
        print(f"ğŸ“‹ Mode: Missing metrics collection")
        print(f"ğŸ“… Period: Last 30 days (metrics only)")
    elif args.all_posts:
        print(f"ğŸ“‹ Mode: All historical posts")
        print(f"ğŸ“… Period: All available data")
    else:
        print(f"ğŸ“‹ Mode: Historical posts + metrics")
        print(f"ğŸ“… Period: {start_date} to {end_date}")
    
    if args.max_posts:
        print(f"ğŸ”¢ Max posts: {args.max_posts}")
    
    print(f"ğŸ“¦ Chunk size: {args.chunk_size}")
    print(f"ğŸ“ˆ Include metrics: {not args.no_metrics and not args.missing_metrics}")
    
    if args.dry_run:
        print(f"ğŸ§ª Mode: DRY RUN (no database writes)")
    
    print("="*60)

def format_result_summary(result) -> dict:
    """çµæœã‚µãƒãƒªãƒ¼ã®æ•´å½¢"""
    return {
        'account_id': result.account_id,
        'collection_type': result.collection_type,
        'date_range': {
            'start_date': result.start_date.isoformat() if result.start_date else None,
            'end_date': result.end_date.isoformat() if result.end_date else None
        },
        'statistics': {
            'total_items': result.total_items,
            'processed_items': result.processed_items,
            'success_items': result.success_items,
            'failed_items': result.failed_items,
            'success_rate': round(result.success_items / result.total_items * 100, 1) if result.total_items > 0 else 0
        },
        'timing': {
            'started_at': result.started_at.isoformat(),
            'completed_at': result.completed_at.isoformat() if result.completed_at else None,
            'duration_seconds': result.duration_seconds,
            'duration_formatted': format_duration(result.duration_seconds)
        },
        'error_message': result.error_message
    }

def format_duration(seconds: float) -> str:
    """ç§’æ•°ã‚’èª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›"""
    if seconds < 60:
        return f"{seconds:.1f}s"
    elif seconds < 3600:
        return f"{seconds/60:.1f}m"
    else:
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        return f"{hours}h {minutes}m"

def print_result_summary(result):
    """å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    print("\n" + "="*60)
    print("ğŸ“Š HISTORICAL COLLECTION SUMMARY")
    print("="*60)
    
    print(f"ğŸ¯ Account: {result.account_id}")
    print(f"ğŸ“‹ Type: {result.collection_type}")
    print(f"â±ï¸  Duration: {format_duration(result.duration_seconds)}")
    print(f"ğŸ“Š Total Items: {result.total_items}")
    print(f"âœ… Processed: {result.processed_items}")
    print(f"ğŸ‰ Success: {result.success_items}")
    print(f"âŒ Failed: {result.failed_items}")
    
    if result.total_items > 0:
        success_rate = (result.success_items / result.total_items) * 100
        print(f"ğŸ“ˆ Success Rate: {success_rate:.1f}%")
    
    if result.error_message:
        print(f"ğŸ’¥ Error: {result.error_message}")
    
    print("="*60)

async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    args = parse_arguments()
    
    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®š
    setup_logging(args.verbose)
    
    # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
    required_env_vars = ['DATABASE_URL']
    missing_vars = [var for var in required_env_vars if not os.getenv(var)]
    if missing_vars:
        logger.error(f"Missing required environment variables: {', '.join(missing_vars)}")
        return 1
    
    logger.info("Starting Instagram historical data collection script")
    
    try:
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ†ã‚¹ãƒˆ
        logger.info("Testing database connection...")
        if not test_connection():
            logger.error("Database connection failed")
            return 1
        logger.info("Database connection successful")
        
        # æ—¥ä»˜ç¯„å›²è¨ˆç®—
        start_date, end_date = calculate_date_range(args)
        
        # åé›†è¨ˆç”»è¡¨ç¤º
        print_collection_plan(args, start_date, end_date)
        
        # ç¢ºèªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ã§ãªã„å ´åˆï¼‰
        if not args.dry_run and not args.yes:
            response = input("\nProceed with historical data collection? (y/N): ")
            if response.lower() != 'y':
                print("Collection cancelled.")
                return 0
        
        # ãƒ‡ãƒ¼ã‚¿åé›†å®Ÿè¡Œ
        collector = create_historical_collector()
        
        if args.missing_metrics:
            logger.info("ğŸš€ Starting missing metrics collection...")
            result = await collector.collect_missing_metrics(
                account_id=args.account,
                days_back=30
            )
        else:
            logger.info("ğŸš€ Starting historical posts collection...")
            result = await collector.collect_historical_posts(
                account_id=args.account,
                start_date=start_date,
                end_date=end_date,
                max_posts=args.max_posts,
                include_metrics=not args.no_metrics,
                chunk_size=args.chunk_size
            )
        
        # çµæœè¡¨ç¤º
        print_result_summary(result)
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        if args.output:
            output_data = format_result_summary(result)
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
            logger.info(f"Results saved to {args.output}")
        
        # å¤±æ•—ã—ãŸå ´åˆã¯ exit code 1
        if result.error_message or result.failed_items > 0:
            logger.warning(f"Collection completed with issues")
            return 1
        
        logger.info("Historical data collection completed successfully")
        return 0
        
    except KeyboardInterrupt:
        logger.info("Collection interrupted by user")
        return 130
    except Exception as e:
        logger.error(f"Critical error in historical data collection: {str(e)}", exc_info=True)
        return 1

def cli_entry_point():
    """CLI ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nâš ï¸  Collection interrupted by user")
        sys.exit(130)
    except Exception as e:
        print(f"\nğŸ’¥ Fatal error: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    cli_entry_point()